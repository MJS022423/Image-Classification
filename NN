import numpy as np
from sympy import flatten

def ReLU(x):
  return np.maximum(0, x)

def ReLu_derivative(x):
  return np.where(x > 0, 1, 0)

def softmax(x):
  
  exp = np.exp(x)
  neg_exp = np.sum(exp)
  
  return (exp - neg_exp) / (exp + exp)


def convolution_operator(input_image, kernel):
  
  input_image = np.array(input_image)
  
  input_IH, input_IW = input_image.shape
  kernel_H, kernel_W = kernel.shape
  
  output_height = input_IH - kernel_H + 1
  output_width = input_IW - kernel_W + 1
  
  output = np.zeros((output_height,output_width))
  
  for i in range(output_height):
    for j in range(output_width):
      output[i, j] = np.sum( input_image[i:i + kernel_H, j:j + kernel_W] * kernel)

  return output

def max_pooling(C_layer, kernel, stride):
  
  C_layer_H, C_layer_W =C_layer.shape
  kernel_H, kernel_W = kernel.shape
  
  output_height = ((C_layer_H - kernel_H) // stride + 1)
  output_width = ((C_layer_W - kernel_W) // stride + 1)
  
  output = np.zeros((output_height,output_width))
  
  for i in range(output_height):
    for j in range(output_width):
      output[i, j] = np.max(C_layer[i * stride:(i * stride) + kernel_H, j * stride:(j * stride) + kernel_W])
                           
  return output

def categorical_cross_entropy(batch_true, batch_prediction):
  
  batch_prediction = np.clip(batch_prediction, 1e-15, 1- 1e-15)
  
  losses = -np.sum(batch_true * np.log( batch_prediction))
  output =  np.mean(losses)
  
  return output

def flatten_size(x):
  
  output = ()
  
  return output

def true_batch():
  
  true_batch = np.array(([1,2,3,4,5,6,7,8,9,0]))
                         
  
  return true_batch 

# initialize weight and biases to disbtribute random weight and bias to synapse that connect each neuron
def initialize(hidden_layer1, hidden_layer2, output_size):
  
  flatten_to_hidden1 = np.random.normal(loc = 0, scale = 1, size = (4, hidden_layer1))
  IH_bias = np.zeros((1, hidden_layer1))
  
  hidden1_to_hidden2  = np.random.normal(loc = 0, scale = 1, size = (hidden_layer1, hidden_layer2))
  HH_bias = np.zeros((1, hidden_layer2))
  
  Hidden2_to_output = np.random.normal(loc = 0, scale = 1, size = (hidden_layer2, output_size))
  HO_bias = np.zeros((1, output_size))
  
  kernel = np.random.normal(loc = 0, scale = 1, size = (3,3))
  
  return flatten_to_hidden1, IH_bias ,hidden1_to_hidden2, HH_bias, Hidden2_to_output, HO_bias, kernel

# forward_propagation
def forward_propagation(input_data, weight_1, bias_1, weight_2, bias_2, weight_3, Bias_3, kernel, stride):
  
  Z1_hidden = convolution_operator(input_data, kernel)
  A1_hidden = ReLU(Z1_hidden)
  
  A2_MP_hidden = max_pooling(A1_hidden, kernel ,stride)
  
  Z3_hidden = convolution_operator(A2_MP_hidden,kernel)
  A3_hidden = ReLU(Z3_hidden)
  
  A4_MP_hidden = max_pooling(A3_hidden, kernel, stride)
  
  A5_flattening = flatten(A4_MP_hidden)
  
  Z6_hidden = np.dot(A5_flattening, weight_1) + bias_1
  A6_hidden = ReLU(Z6_hidden)
  
  Z7_hidden = np.dot(A6_hidden, weight_2) + bias_2
  A7_hidden = ReLU(Z7_hidden)
  
  Z8_hidden = np.dot(A7_hidden, weight_3) + Bias_3
  A8_hidden = softmax(Z8_hidden)
  
  return A1_hidden, A2_MP_hidden, A3_hidden, A4_MP_hidden, A6_hidden, A7_hidden, A8_hidden 

#backward_propagation
def backward_propagation(A8_hidden, A7_hidden, A6_hidden, y_batch_true, weight_3, weight_2):

  output_error = (A8_hidden - y_batch_true)  / A8_hidden.shape[0]
  
  hidden_loss_2 = np.dot(output_error,weight_3.T) *  ReLu_derivative(A7_hidden)
  hidden_loss_1 = np.dot(hidden_loss_2,weight_2.T) *  ReLu_derivative(A6_hidden)
  
  D_weight_1 = np.dot(A6_hidden.T ,hidden_loss_1)
  D_bias_1 = np.sum(hidden_loss_1, axis = 0, keepdims = True )
  print(D_weight_1.shape)
  
  D_weight_2 = np.dot(A7_hidden.T ,hidden_loss_2)
  D_bias_2 = np.sum(hidden_loss_2, axis = 0, keepdims = True )
  print(D_weight_2.shape)
  
  D_weight_3 = np.dot(A8_hidden.T ,output_error)
  D_bias_3 = np.sum(output_error, axis = 0, keepdims = True )
  print(D_weight_3.shape)

  
  return D_weight_1, D_bias_1, D_weight_2, D_bias_2, D_weight_3, D_bias_3

def Convolutional_Neural_network(input_data, epoch):
  
  input_size = 784
  hidden_layer1_size = 10
  hidden_layer2_size = 10
  output_size = 10
  learning_rate = 0.001
  stride = 2
  
  #flatten_size()
  
  Weight_1, Bias_1, Weight_2, Bias_2, Weight_3, Bias_3, kernel = initialize(hidden_layer1_size, hidden_layer2_size, output_size)
  
  for i in range(epoch):
    
    #forward Propagation
    A1_hidden, A2_hidden, A3_hidden, A4_hidden, A6_hidden, A7_hidden, A8_hidden = forward_propagation(input_data ,Weight_1, Bias_1, Weight_2, Bias_2, Weight_3, Bias_3, kernel, stride)
    
    #loss function
    loss  = categorical_cross_entropy(true_batch(), A8_hidden)
    
    #Back Propagation
    D_weight_1, D_bias_1, D_weight_2, D_bias_2, D_weight_3, D_bias_3= backward_propagation(A8_hidden, A7_hidden, A6_hidden, true_batch(), Weight_3, Weight_2)
    
    
    #update Parameter
    Weight_1 -= learning_rate * D_weight_1
    Bias_1 -= learning_rate * D_bias_1
    Weight_2 -= learning_rate * D_weight_2
    Bias_2 -= learning_rate * D_bias_2
    Weight_3 -= learning_rate * D_weight_3
    Bias_3 -= learning_rate * D_bias_3
    
    if(i % 100 == 0):
      print(f"Iteration {i}: Loss {loss:.02f}")
      
  return A1_hidden, A2_hidden, A3_hidden, A4_hidden, A6_hidden, A7_hidden, A8_hidden


input_data = [
    [0.45, 0.13, 0.28, 0.66, 0.56, 0.14, 0.97, 0.22, 0.63, 0.42, 0.89, 0.07, 0.84, 0.37, 0.21, 0.62, 0.76, 0.33, 0.44, 0.19, 0.14, 0.85, 0.65, 0.17, 0.76, 0.53, 0.88, 0.50],
    [0.25, 0.84, 0.10, 0.47, 0.32, 0.49, 0.59, 0.11, 0.53, 0.29, 0.12, 0.74, 0.34, 0.09, 0.70, 0.81, 0.22, 0.90, 0.46, 0.38, 0.21, 0.62, 0.14, 0.72, 0.18, 0.79, 0.55, 0.93],
    [0.31, 0.76, 0.67, 0.13, 0.92, 0.14, 0.51, 0.27, 0.47, 0.15, 0.72, 0.58, 0.86, 0.54, 0.98, 0.44, 0.12, 0.34, 0.55, 0.20, 0.43, 0.18, 0.83, 0.52, 0.11, 0.37, 0.91, 0.60],
    [0.28, 0.15, 0.64, 0.49, 0.43, 0.68, 0.91, 0.32, 0.19, 0.24, 0.69, 0.37, 0.26, 0.57, 0.81, 0.95, 0.23, 0.17, 0.89, 0.41, 0.75, 0.61, 0.06, 0.93, 0.82, 0.30, 0.39, 0.90],
    [0.07, 0.36, 0.88, 0.17, 0.63, 0.49, 0.25, 0.14, 0.50, 0.08, 0.84, 0.77, 0.19, 0.45, 0.67, 0.89, 0.72, 0.10, 0.56, 0.41, 0.31, 0.99, 0.09, 0.20, 0.39, 0.24, 0.74, 0.29],
    [0.96, 0.22, 0.87, 0.59, 0.32, 0.20, 0.27, 0.61, 0.38, 0.04, 0.16, 0.50, 0.31, 0.35, 0.47, 0.79, 0.91, 0.26, 0.71, 0.68, 0.82, 0.53, 0.11, 0.65, 0.75, 0.43, 0.13, 0.62],
    [0.80, 0.50, 0.33, 0.77, 0.16, 0.69, 0.12, 0.59, 0.52, 0.98, 0.44, 0.65, 0.18, 0.60, 0.88, 0.53, 0.72, 0.28, 0.15, 0.43, 0.39, 0.46, 0.86, 0.70, 0.57, 0.10, 0.54, 0.94],
    [0.34, 0.91, 0.29, 0.38, 0.20, 0.92, 0.35, 0.48, 0.14, 0.81, 0.66, 0.40, 0.93, 0.74, 0.63, 0.58, 0.27, 0.22, 0.95, 0.44, 0.67, 0.75, 0.09, 0.60, 0.79, 0.51, 0.36, 0.55],
    [0.19, 0.66, 0.43, 0.85, 0.37, 0.57, 0.30, 0.24, 0.75, 0.49, 0.99, 0.47, 0.92, 0.26, 0.31, 0.11, 0.73, 0.70, 0.78, 0.68, 0.15, 0.13, 0.52, 0.40, 0.08, 0.64, 0.90, 0.42],
    [0.71, 0.57, 0.21, 0.90, 0.33, 0.82, 0.05, 0.25, 0.64, 0.31, 0.45, 0.83, 0.54, 0.09, 0.19, 0.96, 0.59, 0.47, 0.12, 0.80, 0.66, 0.34, 0.55, 0.41, 0.76, 0.23, 0.85, 0.88],
    [0.44, 0.39, 0.50, 0.67, 0.14, 0.58, 0.76, 0.08, 0.18, 0.37, 0.10, 0.96, 0.79, 0.05, 0.70, 0.22, 0.69, 0.73, 0.56, 0.51, 0.60, 0.31, 0.92, 0.38, 0.62, 0.93, 0.43, 0.27],
    [0.07, 0.84, 0.52, 0.26, 0.63, 0.35, 0.97, 0.46, 0.74, 0.13, 0.82, 0.40, 0.48, 0.61, 0.36, 0.15, 0.83, 0.91, 0.11, 0.55, 0.16, 0.29, 0.42, 0.99, 0.53, 0.03, 0.72, 0.94],
    [0.81, 0.49, 0.39, 0.62, 0.85, 0.78, 0.06, 0.67, 0.32, 0.04, 0.59, 0.65, 0.75, 0.44, 0.87, 0.30, 0.21, 0.64, 0.14, 0.48, 0.99, 0.09, 0.25, 0.70, 0.28, 0.86, 0.92, 0.47],
    [0.23, 0.95, 0.54, 0.12, 0.71, 0.32, 0.83, 0.19, 0.55, 0.27, 0.91, 0.38, 0.57, 0.07, 0.63, 0.84, 0.42, 0.10, 0.53, 0.69, 0.03, 0.24, 0.81, 0.17, 0.76, 0.58, 0.40, 0.93],
    [0.15, 0.90, 0.68, 0.41, 0.16, 0.72, 0.13, 0.45, 0.37, 0.79, 0.04, 0.56, 0.28, 0.88, 0.20, 0.34, 0.47, 0.31, 0.12, 0.77, 0.06, 0.93, 0.85, 0.39, 0.29, 0.66, 0.94, 0.26],
    [0.97, 0.10, 0.58, 0.33, 0.21, 0.73, 0.05, 0.92, 0.30, 0.88, 0.44, 0.09, 0.95, 0.82, 0.51, 0.20, 0.87, 0.40, 0.35, 0.75, 0.13, 0.11, 0.59, 0.49, 0.43, 0.98, 0.18, 0.07]
]

Convolutional_Neural_network(input_data, epoch = 1100)